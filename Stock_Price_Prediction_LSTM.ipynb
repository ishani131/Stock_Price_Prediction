{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE3sOKQRmUjj",
        "outputId": "f10eb127-3566-4197-d485-8925f7cf8ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yahoo_fin\n",
            "  Downloading yahoo_fin-0.8.9.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from yahoo_fin) (2.27.1)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from yahoo_fin) (1.4.4)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas->yahoo_fin) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->yahoo_fin) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->yahoo_fin) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->yahoo_fin) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->yahoo_fin) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->yahoo_fin) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->yahoo_fin) (2022.12.7)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyquery\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyppeteer>=0.0.14\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting w3lib\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.4.4)\n",
            "Collecting websockets<11.0,>=10.0\n",
            "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.65.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (6.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->yahoo_fin) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4->requests-html->yahoo_fin) (4.11.2)\n",
            "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.9/dist-packages (from fake-useragent->requests-html->yahoo_fin) (5.12.0)\n",
            "Collecting cssselect>=1.2.0\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.9/dist-packages (from pyquery->requests-html->yahoo_fin) (4.9.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->yahoo_fin) (3.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->bs4->requests-html->yahoo_fin) (2.4)\n",
            "Building wheels for collected packages: bs4, parse, sgmllib3k\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=6bdcd01dcd747166f794af386537b1dc5a51a1a9fc967645164ba8fda6a776f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/2b/cb/099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24589 sha256=e826f0091072cb6c62654373dde5f0a1e66ae4ce64279e8e0558c57b1f053637\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/9c/58/ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=215c0ada6a4cceb0066a5a55e2b786b1c62e323ac13882671864be1bfe811d53\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
            "Successfully built bs4 parse sgmllib3k\n",
            "Installing collected packages: sgmllib3k, pyee, parse, websockets, w3lib, feedparser, cssselect, pyquery, pyppeteer, fake-useragent, bs4, requests-html, yahoo_fin\n",
            "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 feedparser-6.0.10 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 sgmllib3k-1.0.0 w3lib-2.1.1 websockets-10.4 yahoo_fin-0.8.9.1\n"
          ]
        }
      ],
      "source": [
        "pip install yahoo_fin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmrvCF24mw2E",
        "outputId": "6415dbee-4265-49d7-9467-5cfe6c4ffc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pystan~=2.14\n",
            "  Downloading pystan-2.19.1.1.tar.gz (16.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython!=0.25.1,>=0.22 in /usr/local/lib/python3.9/dist-packages (from pystan~=2.14) (0.29.34)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.9/dist-packages (from pystan~=2.14) (1.22.4)\n",
            "Building wheels for collected packages: pystan\n",
            "  Building wheel for pystan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pystan: filename=pystan-2.19.1.1-cp39-cp39-linux_x86_64.whl size=61826428 sha256=8565f1c4fd962e50910d9dec2b346ecbb1fc8124406ae4f49a6e516c70b44f6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/36/bf/7ec7e363f796373cea3eb9ea94e83f5bbbb586d2edbf7e3417\n",
            "Successfully built pystan\n",
            "Installing collected packages: pystan\n",
            "Successfully installed pystan-2.19.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pystan~=2.14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjHwHy0emc4p",
        "outputId": "ed5828ed-a285-4c6d-9725-5bba4f39a7bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2955 sha256=4ce5a29f5c7ad953b05754d6714757589d6f97a89bda96213e7021086281a9ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/e0/3d/9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTPK3jDdoec7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from yahoo_fin import stock_info as si\n",
        "from collections import deque\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ic0pvdiwd2a6",
        "outputId": "b60f0dd2-c42c-4fe7-a8f9-b6c685d8c1a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  open        high         low       close    adjclose  \\\n",
              "1997-05-15    0.121875    0.125000    0.096354    0.097917    0.097917   \n",
              "1997-05-16    0.098438    0.098958    0.085417    0.086458    0.086458   \n",
              "1997-05-19    0.088021    0.088542    0.081250    0.085417    0.085417   \n",
              "1997-05-20    0.086458    0.087500    0.081771    0.081771    0.081771   \n",
              "1997-05-21    0.081771    0.082292    0.068750    0.071354    0.071354   \n",
              "...                ...         ...         ...         ...         ...   \n",
              "2023-03-30  101.550003  103.040001  101.010002  102.000000  102.000000   \n",
              "2023-03-31  102.160004  103.489998  101.949997  103.290001  103.290001   \n",
              "2023-04-03  102.300003  103.290001  101.430000  102.410004  102.410004   \n",
              "2023-04-04  102.750000  104.199997  102.110001  103.949997  103.949997   \n",
              "2023-04-05  103.910004  103.910004  100.750000  101.099998  101.099998   \n",
              "\n",
              "                volume ticker  \n",
              "1997-05-15  1443120000   AMZN  \n",
              "1997-05-16   294000000   AMZN  \n",
              "1997-05-19   122136000   AMZN  \n",
              "1997-05-20   109344000   AMZN  \n",
              "1997-05-21   377064000   AMZN  \n",
              "...                ...    ...  \n",
              "2023-03-30    53633400   AMZN  \n",
              "2023-03-31    56704300   AMZN  \n",
              "2023-04-03    41135700   AMZN  \n",
              "2023-04-04    48662500   AMZN  \n",
              "2023-04-05    45103000   AMZN  \n",
              "\n",
              "[6516 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39821811-f0a1-415c-8336-f1c8bdaae1bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>adjclose</th>\n",
              "      <th>volume</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1997-05-15</th>\n",
              "      <td>0.121875</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.096354</td>\n",
              "      <td>0.097917</td>\n",
              "      <td>0.097917</td>\n",
              "      <td>1443120000</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997-05-16</th>\n",
              "      <td>0.098438</td>\n",
              "      <td>0.098958</td>\n",
              "      <td>0.085417</td>\n",
              "      <td>0.086458</td>\n",
              "      <td>0.086458</td>\n",
              "      <td>294000000</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997-05-19</th>\n",
              "      <td>0.088021</td>\n",
              "      <td>0.088542</td>\n",
              "      <td>0.081250</td>\n",
              "      <td>0.085417</td>\n",
              "      <td>0.085417</td>\n",
              "      <td>122136000</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997-05-20</th>\n",
              "      <td>0.086458</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>109344000</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997-05-21</th>\n",
              "      <td>0.081771</td>\n",
              "      <td>0.082292</td>\n",
              "      <td>0.068750</td>\n",
              "      <td>0.071354</td>\n",
              "      <td>0.071354</td>\n",
              "      <td>377064000</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30</th>\n",
              "      <td>101.550003</td>\n",
              "      <td>103.040001</td>\n",
              "      <td>101.010002</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>53633400</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31</th>\n",
              "      <td>102.160004</td>\n",
              "      <td>103.489998</td>\n",
              "      <td>101.949997</td>\n",
              "      <td>103.290001</td>\n",
              "      <td>103.290001</td>\n",
              "      <td>56704300</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-03</th>\n",
              "      <td>102.300003</td>\n",
              "      <td>103.290001</td>\n",
              "      <td>101.430000</td>\n",
              "      <td>102.410004</td>\n",
              "      <td>102.410004</td>\n",
              "      <td>41135700</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-04</th>\n",
              "      <td>102.750000</td>\n",
              "      <td>104.199997</td>\n",
              "      <td>102.110001</td>\n",
              "      <td>103.949997</td>\n",
              "      <td>103.949997</td>\n",
              "      <td>48662500</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-05</th>\n",
              "      <td>103.910004</td>\n",
              "      <td>103.910004</td>\n",
              "      <td>100.750000</td>\n",
              "      <td>101.099998</td>\n",
              "      <td>101.099998</td>\n",
              "      <td>45103000</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6516 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39821811-f0a1-415c-8336-f1c8bdaae1bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39821811-f0a1-415c-8336-f1c8bdaae1bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39821811-f0a1-415c-8336-f1c8bdaae1bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "si.get_data(\"amzn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGZ_Idbzogn7"
      },
      "outputs": [],
      "source": [
        "# set seed, so we can get the same results after rerunning several times\n",
        "np.random.seed(314)\n",
        "tf.random.set_seed(314)\n",
        "random.seed(314)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t8_Qbshojeu"
      },
      "outputs": [],
      "source": [
        "def shuffle_in_unison(a, b):\n",
        "    # shuffle two arrays in the same way\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(b)\n",
        "\n",
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
        "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "  # see if ticker is already a loaded stock from yahoo finance\n",
        "    if isinstance(ticker, str):\n",
        "        # load it from yahoo_fin library\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        # already loaded, use it directly\n",
        "        df = ticker\n",
        "    else:\n",
        "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
        "    # add date as a column\n",
        "    if \"date\" not in df.columns:\n",
        "        df[\"date\"] = df.index\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
        "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
        "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    if split_by_date:\n",
        "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
        "        train_samples = int((1 - test_size) * len(X))\n",
        "        result[\"X_train\"] = X[:train_samples]\n",
        "        result[\"y_train\"] = y[:train_samples]\n",
        "        result[\"X_test\"]  = X[train_samples:]\n",
        "        result[\"y_test\"]  = y[train_samples:]\n",
        "        if shuffle:\n",
        "            # shuffle the datasets for training (if shuffle parameter is set)\n",
        "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
        "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
        "    else:    \n",
        "        # split the dataset randomly\n",
        "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
        "                                                                                test_size=test_size, shuffle=shuffle)\n",
        "    # get the list of test set dates\n",
        "    dates = result[\"X_test\"][:, -1, -1]\n",
        "    # retrieve test features from the original dataframe\n",
        "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
        "    # remove duplicated dates in the testing dataframe\n",
        "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
        "    # remove dates from the training/testing sets & convert to float32\n",
        "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaK1BspvpKB2"
      },
      "outputs": [],
      "source": [
        "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=True):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzTjmH-mpSE2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Window size or the sequence length\n",
        "N_STEPS = 50\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 15\n",
        "# whether to scale feature columns & output price as well\n",
        "SCALE = True\n",
        "scale_str = f\"sc-{int(SCALE)}\"\n",
        "# whether to shuffle the dataset\n",
        "SHUFFLE = True\n",
        "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
        "# whether to split the training/testing set by date\n",
        "SPLIT_BY_DATE = False\n",
        "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 2\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "# whether to use bidirectional RNNs\n",
        "BIDIRECTIONAL = False\n",
        "### training parameters\n",
        "# mean absolute error loss\n",
        "# LOSS = \"mae\"\n",
        "# huber loss\n",
        "LOSS = \"huber_loss\"\n",
        "OPTIMIZER = \"adam\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "# Amazon stock market\n",
        "ticker = \"AMZN\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save, making it as unique as possible based on parameters\n",
        "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
        "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "if BIDIRECTIONAL:\n",
        "    model_name += \"-b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-tLTYIspbAJ"
      },
      "outputs": [],
      "source": [
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CitprjBjpgLC",
        "outputId": "0bb682ca-8a1c-4953-8ad7-01e670e2f1d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0272\n",
            "Epoch 1: val_loss improved from inf to 0.00042, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 185s 2s/step - loss: 0.0020 - mean_absolute_error: 0.0272 - val_loss: 4.2249e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 2/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 7.9369e-04 - mean_absolute_error: 0.0204\n",
            "Epoch 2: val_loss improved from 0.00042 to 0.00038, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 153s 2s/step - loss: 7.9369e-04 - mean_absolute_error: 0.0204 - val_loss: 3.8432e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 3/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 6.5402e-04 - mean_absolute_error: 0.0174\n",
            "Epoch 3: val_loss improved from 0.00038 to 0.00038, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 152s 2s/step - loss: 6.5402e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8059e-04 - val_mean_absolute_error: 0.0135\n",
            "Epoch 4/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 7.9974e-04 - mean_absolute_error: 0.0190\n",
            "Epoch 4: val_loss improved from 0.00038 to 0.00037, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 154s 2s/step - loss: 7.9974e-04 - mean_absolute_error: 0.0190 - val_loss: 3.7407e-04 - val_mean_absolute_error: 0.0122\n",
            "Epoch 5/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 6.4127e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 5: val_loss improved from 0.00037 to 0.00036, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 151s 2s/step - loss: 6.4127e-04 - mean_absolute_error: 0.0171 - val_loss: 3.5941e-04 - val_mean_absolute_error: 0.0128\n",
            "Epoch 6/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 7.0969e-04 - mean_absolute_error: 0.0185\n",
            "Epoch 6: val_loss did not improve from 0.00036\n",
            "81/81 [==============================] - 151s 2s/step - loss: 7.0969e-04 - mean_absolute_error: 0.0185 - val_loss: 4.8617e-04 - val_mean_absolute_error: 0.0163\n",
            "Epoch 7/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 6.5117e-04 - mean_absolute_error: 0.0178\n",
            "Epoch 7: val_loss did not improve from 0.00036\n",
            "81/81 [==============================] - 148s 2s/step - loss: 6.5117e-04 - mean_absolute_error: 0.0178 - val_loss: 4.6693e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 8/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.8506e-04 - mean_absolute_error: 0.0165\n",
            "Epoch 8: val_loss improved from 0.00036 to 0.00033, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 151s 2s/step - loss: 5.8506e-04 - mean_absolute_error: 0.0165 - val_loss: 3.3241e-04 - val_mean_absolute_error: 0.0116\n",
            "Epoch 9/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.6063e-04 - mean_absolute_error: 0.0163\n",
            "Epoch 9: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.6063e-04 - mean_absolute_error: 0.0163 - val_loss: 4.2882e-04 - val_mean_absolute_error: 0.0149\n",
            "Epoch 10/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 6.3177e-04 - mean_absolute_error: 0.0177\n",
            "Epoch 10: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 147s 2s/step - loss: 6.3177e-04 - mean_absolute_error: 0.0177 - val_loss: 3.6158e-04 - val_mean_absolute_error: 0.0120\n",
            "Epoch 11/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.7114e-04 - mean_absolute_error: 0.0169\n",
            "Epoch 11: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.7114e-04 - mean_absolute_error: 0.0169 - val_loss: 3.5264e-04 - val_mean_absolute_error: 0.0120\n",
            "Epoch 12/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.7676e-04 - mean_absolute_error: 0.0172\n",
            "Epoch 12: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.7676e-04 - mean_absolute_error: 0.0172 - val_loss: 3.3567e-04 - val_mean_absolute_error: 0.0116\n",
            "Epoch 13/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.7914e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 13: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 146s 2s/step - loss: 5.7914e-04 - mean_absolute_error: 0.0171 - val_loss: 3.4042e-04 - val_mean_absolute_error: 0.0130\n",
            "Epoch 14/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.8850e-04 - mean_absolute_error: 0.0179\n",
            "Epoch 14: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 149s 2s/step - loss: 5.8850e-04 - mean_absolute_error: 0.0179 - val_loss: 3.4490e-04 - val_mean_absolute_error: 0.0132\n",
            "Epoch 15/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 6.3511e-04 - mean_absolute_error: 0.0181\n",
            "Epoch 15: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 147s 2s/step - loss: 6.3511e-04 - mean_absolute_error: 0.0181 - val_loss: 4.8693e-04 - val_mean_absolute_error: 0.0149\n",
            "Epoch 16/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 6.1632e-04 - mean_absolute_error: 0.0177\n",
            "Epoch 16: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 148s 2s/step - loss: 6.1632e-04 - mean_absolute_error: 0.0177 - val_loss: 4.6011e-04 - val_mean_absolute_error: 0.0152\n",
            "Epoch 17/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.5107e-04 - mean_absolute_error: 0.0170\n",
            "Epoch 17: val_loss improved from 0.00033 to 0.00033, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 148s 2s/step - loss: 5.5107e-04 - mean_absolute_error: 0.0170 - val_loss: 3.3166e-04 - val_mean_absolute_error: 0.0116\n",
            "Epoch 18/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.5382e-04 - mean_absolute_error: 0.0167\n",
            "Epoch 18: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 148s 2s/step - loss: 5.5382e-04 - mean_absolute_error: 0.0167 - val_loss: 3.4249e-04 - val_mean_absolute_error: 0.0123\n",
            "Epoch 19/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.4153e-04 - mean_absolute_error: 0.0168\n",
            "Epoch 19: val_loss did not improve from 0.00033\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.4153e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8752e-04 - val_mean_absolute_error: 0.0138\n",
            "Epoch 20/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.7912e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 20: val_loss improved from 0.00033 to 0.00032, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 148s 2s/step - loss: 5.7912e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1609e-04 - val_mean_absolute_error: 0.0112\n",
            "Epoch 21/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.3991e-04 - mean_absolute_error: 0.0162\n",
            "Epoch 21: val_loss did not improve from 0.00032\n",
            "81/81 [==============================] - 146s 2s/step - loss: 5.3991e-04 - mean_absolute_error: 0.0162 - val_loss: 3.3872e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 22/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.7360e-04 - mean_absolute_error: 0.0176\n",
            "Epoch 22: val_loss did not improve from 0.00032\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.7360e-04 - mean_absolute_error: 0.0176 - val_loss: 3.8740e-04 - val_mean_absolute_error: 0.0159\n",
            "Epoch 23/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.3817e-04 - mean_absolute_error: 0.0166\n",
            "Epoch 23: val_loss did not improve from 0.00032\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.3817e-04 - mean_absolute_error: 0.0166 - val_loss: 3.1941e-04 - val_mean_absolute_error: 0.0112\n",
            "Epoch 24/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.4850e-04 - mean_absolute_error: 0.0167\n",
            "Epoch 24: val_loss did not improve from 0.00032\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.4850e-04 - mean_absolute_error: 0.0167 - val_loss: 3.8959e-04 - val_mean_absolute_error: 0.0166\n",
            "Epoch 25/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.3721e-04 - mean_absolute_error: 0.0169\n",
            "Epoch 25: val_loss did not improve from 0.00032\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.3721e-04 - mean_absolute_error: 0.0169 - val_loss: 4.1976e-04 - val_mean_absolute_error: 0.0131\n",
            "Epoch 26/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.0425e-04 - mean_absolute_error: 0.0162\n",
            "Epoch 26: val_loss improved from 0.00032 to 0.00031, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 148s 2s/step - loss: 5.0425e-04 - mean_absolute_error: 0.0162 - val_loss: 3.1318e-04 - val_mean_absolute_error: 0.0114\n",
            "Epoch 27/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.5193e-04 - mean_absolute_error: 0.0152\n",
            "Epoch 27: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 4.5193e-04 - mean_absolute_error: 0.0152 - val_loss: 3.3617e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 28/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.1930e-04 - mean_absolute_error: 0.0165\n",
            "Epoch 28: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.1930e-04 - mean_absolute_error: 0.0165 - val_loss: 4.2079e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 29/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.9240e-04 - mean_absolute_error: 0.0162\n",
            "Epoch 29: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 4.9240e-04 - mean_absolute_error: 0.0162 - val_loss: 3.3522e-04 - val_mean_absolute_error: 0.0116\n",
            "Epoch 30/50\n",
            "80/81 [============================>.] - ETA: 1s - loss: 4.8773e-04 - mean_absolute_error: 0.0158\n",
            "Epoch 30: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 148s 2s/step - loss: 4.8985e-04 - mean_absolute_error: 0.0158 - val_loss: 3.1687e-04 - val_mean_absolute_error: 0.0113\n",
            "Epoch 31/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.5016e-04 - mean_absolute_error: 0.0173\n",
            "Epoch 31: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.5016e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3417e-04 - val_mean_absolute_error: 0.0121\n",
            "Epoch 32/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.1367e-04 - mean_absolute_error: 0.0164\n",
            "Epoch 32: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 146s 2s/step - loss: 5.1367e-04 - mean_absolute_error: 0.0164 - val_loss: 3.2269e-04 - val_mean_absolute_error: 0.0112\n",
            "Epoch 33/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.5864e-04 - mean_absolute_error: 0.0172\n",
            "Epoch 33: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 148s 2s/step - loss: 5.5864e-04 - mean_absolute_error: 0.0172 - val_loss: 3.9889e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 34/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.7227e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 34: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 148s 2s/step - loss: 5.7227e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2813e-04 - val_mean_absolute_error: 0.0120\n",
            "Epoch 35/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.0681e-04 - mean_absolute_error: 0.0160\n",
            "Epoch 35: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 151s 2s/step - loss: 5.0681e-04 - mean_absolute_error: 0.0160 - val_loss: 3.2168e-04 - val_mean_absolute_error: 0.0128\n",
            "Epoch 36/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.3970e-04 - mean_absolute_error: 0.0167\n",
            "Epoch 36: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 151s 2s/step - loss: 5.3970e-04 - mean_absolute_error: 0.0167 - val_loss: 3.5373e-04 - val_mean_absolute_error: 0.0122\n",
            "Epoch 37/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.7854e-04 - mean_absolute_error: 0.0156\n",
            "Epoch 37: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 148s 2s/step - loss: 4.7854e-04 - mean_absolute_error: 0.0156 - val_loss: 3.1632e-04 - val_mean_absolute_error: 0.0121\n",
            "Epoch 38/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.7383e-04 - mean_absolute_error: 0.0158\n",
            "Epoch 38: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 148s 2s/step - loss: 4.7383e-04 - mean_absolute_error: 0.0158 - val_loss: 6.1288e-04 - val_mean_absolute_error: 0.0167\n",
            "Epoch 39/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.3620e-04 - mean_absolute_error: 0.0168\n",
            "Epoch 39: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.3620e-04 - mean_absolute_error: 0.0168 - val_loss: 3.4502e-04 - val_mean_absolute_error: 0.0126\n",
            "Epoch 40/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.6287e-04 - mean_absolute_error: 0.0156\n",
            "Epoch 40: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 148s 2s/step - loss: 4.6287e-04 - mean_absolute_error: 0.0156 - val_loss: 3.1362e-04 - val_mean_absolute_error: 0.0112\n",
            "Epoch 41/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.8152e-04 - mean_absolute_error: 0.0158\n",
            "Epoch 41: val_loss improved from 0.00031 to 0.00031, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 148s 2s/step - loss: 4.8152e-04 - mean_absolute_error: 0.0158 - val_loss: 3.1183e-04 - val_mean_absolute_error: 0.0124\n",
            "Epoch 42/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.2575e-04 - mean_absolute_error: 0.0172\n",
            "Epoch 42: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.2575e-04 - mean_absolute_error: 0.0172 - val_loss: 3.4853e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 43/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.8445e-04 - mean_absolute_error: 0.0163\n",
            "Epoch 43: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 146s 2s/step - loss: 4.8445e-04 - mean_absolute_error: 0.0163 - val_loss: 3.5297e-04 - val_mean_absolute_error: 0.0129\n",
            "Epoch 44/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.7967e-04 - mean_absolute_error: 0.0162\n",
            "Epoch 44: val_loss improved from 0.00031 to 0.00031, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 147s 2s/step - loss: 4.7967e-04 - mean_absolute_error: 0.0162 - val_loss: 3.0888e-04 - val_mean_absolute_error: 0.0117\n",
            "Epoch 45/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.7853e-04 - mean_absolute_error: 0.0160\n",
            "Epoch 45: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 4.7853e-04 - mean_absolute_error: 0.0160 - val_loss: 3.1743e-04 - val_mean_absolute_error: 0.0135\n",
            "Epoch 46/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.7660e-04 - mean_absolute_error: 0.0160\n",
            "Epoch 46: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 4.7660e-04 - mean_absolute_error: 0.0160 - val_loss: 3.1772e-04 - val_mean_absolute_error: 0.0115\n",
            "Epoch 47/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.0645e-04 - mean_absolute_error: 0.0163\n",
            "Epoch 47: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.0645e-04 - mean_absolute_error: 0.0163 - val_loss: 3.2350e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 48/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 4.7245e-04 - mean_absolute_error: 0.0159\n",
            "Epoch 48: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 148s 2s/step - loss: 4.7245e-04 - mean_absolute_error: 0.0159 - val_loss: 3.3804e-04 - val_mean_absolute_error: 0.0129\n",
            "Epoch 49/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.0067e-04 - mean_absolute_error: 0.0164\n",
            "Epoch 49: val_loss improved from 0.00031 to 0.00031, saving model to results/2023-04-06_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256-b.h5\n",
            "81/81 [==============================] - 147s 2s/step - loss: 5.0067e-04 - mean_absolute_error: 0.0164 - val_loss: 3.0876e-04 - val_mean_absolute_error: 0.0112\n",
            "Epoch 50/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 5.1638e-04 - mean_absolute_error: 0.0170\n",
            "Epoch 50: val_loss did not improve from 0.00031\n",
            "81/81 [==============================] - 148s 2s/step - loss: 5.1638e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1072e-04 - val_mean_absolute_error: 0.0123\n"
          ]
        }
      ],
      "source": [
        "# load the data\n",
        "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
        "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
        "                feature_columns=FEATURE_COLUMNS)\n",
        "# save the dataframe\n",
        "data[\"df\"].to_csv(ticker_data_filename)\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "# train the model and save the weights whenever we see \n",
        "# a new optimal model using ModelCheckpoint\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kajzukaxq2hS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graph(test_df):\n",
        "    \"\"\"\n",
        "    This function plots true close price along with predicted close price\n",
        "    with blue and red colors respectively\n",
        "    \"\"\"\n",
        "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
        "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gg051o5uNrf"
      },
      "outputs": [],
      "source": [
        "def get_final_df(model, data):\n",
        "    \"\"\"\n",
        "    This function takes the `model` and `data` dict to \n",
        "    construct a final dataframe that includes the features along \n",
        "    with true and predicted prices of the testing dataset\n",
        "    \"\"\"\n",
        "    # if predicted future price is higher than the current, \n",
        "    # then calculate the true future price minus the current price, to get the buy profit\n",
        "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
        "    # if the predicted future price is lower than the current price,\n",
        "    # then subtract the true future price from the current price\n",
        "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_test = data[\"y_test\"]\n",
        "    # perform prediction and get prices\n",
        "    y_pred = model.predict(X_test)\n",
        "    if SCALE:\n",
        "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    test_df = data[\"test_df\"]\n",
        "    # add predicted future prices to the dataframe\n",
        "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
        "    # add true future prices to the dataframe\n",
        "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
        "    # sort the dataframe by date\n",
        "    test_df.sort_index(inplace=True)\n",
        "    final_df = test_df\n",
        "    # add the buy profit column\n",
        "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
        "                                    final_df[\"adjclose\"], \n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    # add the sell profit column\n",
        "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
        "                                    final_df[\"adjclose\"], \n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kOAFqTbq-zq"
      },
      "outputs": [],
      "source": [
        "def predict(model, data):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    if SCALE:\n",
        "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "    else:\n",
        "        predicted_price = prediction[0][0]\n",
        "    return predicted_price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10nZIBd4uZE3"
      },
      "outputs": [],
      "source": [
        "# load optimal model weights from results folder\n",
        "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
        "model.load_weights(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feq6U2Tbuahq"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "if SCALE:\n",
        "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
        "else:\n",
        "    mean_absolute_error = mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhqy4fjXuiNU",
        "outputId": "bc34ef05-3b71-46cd-cd63-282ecb6c7463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 13s 265ms/step\n"
          ]
        }
      ],
      "source": [
        "# get the final dataframe for the testing set\n",
        "final_df = get_final_df(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a_h_z1hukvE",
        "outputId": "fcab305f-9adc-483a-9300-ed388d6533ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 53ms/step\n"
          ]
        }
      ],
      "source": [
        "# predict the future price\n",
        "future_price = predict(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8mnOlsuuolu"
      },
      "outputs": [],
      "source": [
        "# we calculate the accuracy by counting the number of positive profits\n",
        "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
        "# calculating total buy & sell profit\n",
        "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
        "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
        "# total profit by adding sell & buy together\n",
        "total_profit = total_buy_profit + total_sell_profit\n",
        "# dividing total profit by number of testing samples (number of trades)\n",
        "profit_per_trade = total_profit / len(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOfJ6SmnurCV",
        "outputId": "bb25b3a7-e7ec-4400-8996-e5c8701644ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Future price after 15 days is 104.01$\n",
            "huber_loss loss: 0.00030875889933668077\n",
            "Mean Absolute Error: 2.161556572600019\n",
            "Accuracy score: 0.5213013168086754\n",
            "Total buy profit: 582.354406118393\n",
            "Total sell profit: 30.90719904005524\n",
            "Total profit: 613.2616051584482\n",
            "Profit per trade: 0.47502835411188865\n"
          ]
        }
      ],
      "source": [
        "# printing metrics\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
        "print(f\"{LOSS} loss:\", loss)\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
        "print(\"Accuracy score:\", accuracy_score)\n",
        "print(\"Total buy profit:\", total_buy_profit)\n",
        "print(\"Total sell profit:\", total_sell_profit)\n",
        "print(\"Total profit:\", total_profit)\n",
        "print(\"Profit per trade:\", profit_per_trade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "og2HB25Puv27",
        "outputId": "066ffc45-1f03-41fb-d2b5-907247e79579"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot true/pred prices graph\n",
        "plot_graph(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh0SCAINu5Rm",
        "outputId": "2b4db3a2-fe43-4758-c7da-181b2e1e146d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  open        high         low       close    adjclose  \\\n",
            "2023-01-06   83.029999   86.400002   81.430000   86.080002   86.080002   \n",
            "2023-01-10   87.570000   90.190002   87.290001   89.870003   89.870003   \n",
            "2023-01-17   98.680000   98.889999   95.730003   96.050003   96.050003   \n",
            "2023-01-25   92.559998   97.239998   91.519997   97.180000   97.180000   \n",
            "2023-02-02  110.250000  114.000000  108.879997  112.910004  112.910004   \n",
            "2023-02-13   97.849998   99.680000   96.910004   99.540001   99.540001   \n",
            "2023-02-24   93.529999   94.139999   92.320000   93.500000   93.500000   \n",
            "2023-02-27   94.279999   94.779999   93.139999   93.760002   93.760002   \n",
            "2023-03-01   93.870003   94.680000   91.589996   92.169998   92.169998   \n",
            "2023-03-09   93.680000   96.209999   92.180000   92.250000   92.250000   \n",
            "\n",
            "               volume ticker  adjclose_15  true_adjclose_15  buy_profit  \\\n",
            "2023-01-06   83303400   AMZN    89.968590        100.550003   14.470001   \n",
            "2023-01-10   67756600   AMZN    92.261787        105.150002   15.279999   \n",
            "2023-01-17   72755000   AMZN    98.886009        102.110001    6.059998   \n",
            "2023-01-25   94261600   AMZN    97.906120        101.160004    3.980003   \n",
            "2023-02-02  158154200   AMZN   108.442245         93.500000    0.000000   \n",
            "2023-02-13   52841500   AMZN    99.426750         93.550003    0.000000   \n",
            "2023-02-24   57053800   AMZN    95.368156         98.949997    5.449997   \n",
            "2023-02-27   47470300   AMZN    94.951248         97.709999    3.949997   \n",
            "2023-03-01   52299500   AMZN    94.095627         98.699997    6.529999   \n",
            "2023-03-09   56218700   AMZN    94.179855        102.000000    9.750000   \n",
            "\n",
            "            sell_profit  \n",
            "2023-01-06     0.000000  \n",
            "2023-01-10     0.000000  \n",
            "2023-01-17     0.000000  \n",
            "2023-01-25     0.000000  \n",
            "2023-02-02    19.410004  \n",
            "2023-02-13     5.989998  \n",
            "2023-02-24     0.000000  \n",
            "2023-02-27     0.000000  \n",
            "2023-03-01     0.000000  \n",
            "2023-03-09     0.000000  \n"
          ]
        }
      ],
      "source": [
        "#@title Default title text\n",
        "print(final_df.tail(10))\n",
        "# save the final dataframe to csv-results folder\n",
        "csv_results_folder = \"csv-results\"\n",
        "if not os.path.isdir(csv_results_folder):\n",
        "    os.mkdir(csv_results_folder)\n",
        "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
        "final_df.to_csv(csv_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}